{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nRandom Search vs. Bayesian Optimization\n=======================================\n\nIn this section, we demonstrate the behaviors of random search and Bayesian optimization\nin a simple simulation environment.\n\nCreate a Reward Function for Toy Experiments\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n- Import the packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Generate the simulated reward as a mixture of 2 gaussians:\n\nInput Space `x = [0: 99], y = [0: 99]`.\nThe rewards is a combination of 2 gaussians as shown in the following figure:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gaussian2d(x, y, x0, y0, xalpha, yalpha, A): \n    return A * np.exp( -((x-x0)/xalpha)**2 -((y-y0)/yalpha)**2) \n\nx, y = np.linspace(0, 99, 100), np.linspace(0, 99, 100) \nX, Y = np.meshgrid(x, y)\n\nZ = np.zeros(X.shape) \nps = [(20, 70, 35, 40, 1),\n      (80, 40, 20, 20, 0.7)]\nfor p in ps:\n    Z += gaussian2d(X, Y, *p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Visualize the reward space:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nax = fig.gca(projection='3d') \nax.plot_surface(X, Y, Z, cmap='plasma') \nax.set_zlim(0,np.max(Z)+2)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Training Function\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe can simply define an AutoTorch searchable function with a decorator `at.gargs`.\nThe `reporter` is used to communicate with AutoTorch search and scheduling algorithms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import autotorch as at\n\n@at.args(\n    x=at.Int(0, 99),\n    y=at.Int(0, 99),\n)\ndef toy_simulation(args, reporter):\n    x, y = args.x, args.y\n    reporter(accuracy=Z[y][x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random Search\n~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "random_scheduler = at.scheduler.FIFOScheduler(toy_simulation,\n                                              resource={'num_cpus': 1, 'num_gpus': 0},\n                                              num_trials=30,\n                                              reward_attr=\"accuracy\",\n                                              resume=False)\nrandom_scheduler.run()\nrandom_scheduler.join_jobs()\nprint('Best config: {}, best reward: {}'.format(random_scheduler.get_best_config(), random_scheduler.get_best_reward()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bayesian Optimization\n~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bayesopt_scheduler = at.scheduler.FIFOScheduler(toy_simulation,\n                                                searcher='bayesopt',\n                                                resource={'num_cpus': 1, 'num_gpus': 0},\n                                                num_trials=30,\n                                                reward_attr=\"accuracy\",\n                                                resume=False)\nbayesopt_scheduler.run()\nbayesopt_scheduler.join_jobs()\nprint('Best config: {}, best reward: {}'.format(bayesopt_scheduler.get_best_config(), bayesopt_scheduler.get_best_reward()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the performance\n~~~~~~~~~~~~~~~~~~~~~~~\n\nGet the result history:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results_bayes = [v[0]['accuracy'] for v in bayesopt_scheduler.training_history.values()]\nresults_random = [v[0]['accuracy'] for v in random_scheduler.training_history.values()]\n\nfig = plt.figure()\nplt.plot(range(len(results_random)), results_random, range(len(results_bayes)), results_bayes)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Advance Usage for Bayesian Optimization\n---------------------------------------\n\nFor some special cases, not all configurations are valid for the requirement.\nInstead of falling back to random search, we can pre-generate a set of valid\nconfigurations using random search, and accelerate the HPO using Bayesian\nOptimization. The key idea is fitting GP model using observed data points, and\nusing acqusition function to re-rank the pending configurations.\n\n- Define valid condiction\n\nWe require x or y to be an even number\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def is_valid_config(config):\n    return config['x'] % 2 == 0 or config['y'] % 2 == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Pre-generate configurations using random searcher\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "random_searcher = at.searcher.RandomSearcher(toy_simulation.cs)\n\nlazy_configs = []\nvalid_cnt = 0\n\nwhile valid_cnt < 500:\n    config = random_searcher.get_config()\n    if is_valid_config(config):\n        valid_cnt += 1\n        lazy_configs.append(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Initialize lazy configurations with Bayesian optimization\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lazy_bayes = at.searcher.BayesOptSearcher(toy_simulation.cs, lazy_configs=lazy_configs)\n\nlazy_scheduler = at.scheduler.FIFOScheduler(toy_simulation,\n                                            searcher=lazy_bayes,\n                                            resource={'num_cpus': 1, 'num_gpus': 0},\n                                            num_trials=20,\n                                            reward_attr=\"accuracy\",\n                                            resume=False)\nlazy_scheduler.run()\nlazy_scheduler.join_jobs()\nprint('Best config: {}, best reward: {}'.format(lazy_scheduler.get_best_config(), lazy_scheduler.get_best_reward()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Plot the training curve\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nresults_lazy = [v[0]['accuracy'] for v in lazy_scheduler.training_history.values()]\nplt.plot(range(len(results_lazy)), results_lazy)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}